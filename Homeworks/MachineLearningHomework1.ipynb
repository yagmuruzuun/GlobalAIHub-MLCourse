{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearningHomework1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Tqlr_AbjXc"
      },
      "source": [
        "<h2> 1) How would you define Machine Learning? </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kk_qD6abvru"
      },
      "source": [
        "<b>Machine Learning</b> is a sub-area of artificial intelligence (AI). It is a process of letting machines learn from the data to analyze the relationship between features. Here, the aim is building the model using data and answers unlike traditional programming. Machine Learning enables algorithms to identify patterns within data without being especially programmed. It also uses those patterns to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIbT6rhwcB5v"
      },
      "source": [
        "<h2>2) What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these and give brief information about how they work.</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9lq7ekrcSLc"
      },
      "source": [
        "<b>In Supervised Learning,</b> the possible outcomes are defined. We have labeled data. Input variables(x) and output variable(y) is known and algorithms tries to analyze the relationship between input and output. We can divide our data as train, test, validation sets. Algorithms learn from the training dataset and tries to predict the output. Score of the model is evaluated with test dataset. The goal is to understand the relationship so well that when a new input data(x) is given, the output variable(Y) can be predicted well for the new data. If the output label is categorical, classification algorithms are used. If the output label is numerical, regression algorithms are used.<br><br>Possible algorithms are: <br>-Linear Regression: It is used for regression problems.<br>-Logistic Regression: It is used for binary classification problems.<br>-Decision Tree: It is used for regression and classification problems.<br><br>\n",
        "<b>In Unsupervised Learning,</b> no labels are provided. Algorithm tries to detect structure from the input data. We only have input data (X), no corresponding output variable(y). There may be plenty possible clustering, no right or wrong. In Unsupervised Learning, model tries to discover hidden patterns and find relationship. It also does feature extraction and relationships by grouping data into clusters.<br><br>Possible algorithms are:<br>-K-means clustering: It is used for clustering problems.<br>-Hierarchical clustering: It is used for clustering problems. <br>-Principal Components Analysis (PCA): It is used for dimensionality reduction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RRkHfWxf9YC"
      },
      "source": [
        "<h2>3) What are the train, test and validation sets? Why should we use them?</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9j8GQSihBIv"
      },
      "source": [
        "<b>Training Dataset:</b> The sample of data used to fit/train the model.<br><b>Validation Dataset:</b> The sample of data used to tune the model hyperparameters.<br><b>Test Dataset:</b> The sample of data used to evaluate final performance of the model.<br><br>For example, when overfitting occurs, the algorithm will be accurate on the training dataset, but highly inaccurate on the testing dataset. To overcome this, another separate validation set is used. The purpose of this is to teach the algorithm with the training set and optimize the hyperparameters based on the accuracy of the validation data set. Once a final model is created based on the training and validation test sets, it is applied to the testing set for a final unbiased evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6LkiUC1hNZi"
      },
      "source": [
        "<h2>4) What are the main preprocessing steps? Explain them in detail. Why do we need to prepare our data?</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krgP2uQ2he2B"
      },
      "source": [
        "After gathering the data, we have to prepare it for the use in our machine learning training. We have to clean the data and transform it in a way that model can understand. To analyze the data, Exploratory Data Analysis should be done. <br><b><u><br>PreProcessing Steps:</u></b><br><br><b>1. Duplicate Values:</b><br>Mostly we remove duplicate values so model would not give advantage or bias to that specific value.<br><br><b>2. Imbalanced Data:</b><br>An Imbalanced dataset is one where the number of instances of a class(es) are significantly higher than another class(es), thus leading to an imbalance and creating rarer class(es). We should use undersampling or oversampling methods.<br><br><b>3. Missing Values:</b><br>When analyzing dataset, we can see some values are missing. It is important to identify and handle missing values so that our model can learn better. There are some ways to handle missing values.<br>-Eliminate missing values: If our data is big enough and proportion of missing values are relatively small, we can consider dropping them. <br>-Filling with mean, mode or median: We can fill missing values with proper statistical metric. These metrics can be mean, mode or median. This method works for numeric data.<br><br><b>4. Outlier Detection:</b><br>Outliers are extreme values that deviate from other observations on data. Outliers in a dataset may be occurred because of data entry errors, measurement errors etc. Removing them would be an option, but in some cases, we should not drop them because they may be not outliers, they may affect model performance. We can detect these outliers by using Standart Deviation,Box Plots/IQR Calculation,Isolation Forest…<br><br><b>5.Bucketing (Binning):</b><br>Data binning, bucketing is a data pre-processing method used to minimize the effects of small observation errors (noisy data). The original data values are divided into small intervals known as bins and then they are replaced by a general value calculated for that bin.<br><br><b>6. Feature Encoding:</b><br>Feature encoding is basically transforming data so it can be easily accepted and understood by machine learning model as input. We have to transform our categorical values to numerical values since machine learning models can only work with numerical values.<br>-For Nominal data: One-Hot Encoding method can be used.<br>-For Ordinal data: Label encoding method can be used.<br><br><b>7. Feature Scaling:</b><br>It is a method to bring independent variables of a dataset within a specific range. Scaling limits the range of variables so we can compare them better.<br>Standardization: It transforms data to have a mean of zero and a standard deviation of 1.$$  X_{new} = \\frac{X-\\mu}{\\sigma} $$\n",
        "Normalization: It transforms data to have a values between 0 and 1.$$X_{new} = \\frac{X-X_{min}}{X_{max} - X_{min}} $$<br><b>8. Splitting Dataset:</b><br>\n",
        "We have to split our dataset into 2 (Train and Test) or 3  as (Train,Validation,Test) sets so we can fit, test and validate our data during machine learning process.<br>Common split ratios are:<br>70/30 (Train/Test)<br>60/20/20 (Train/Test/Validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfUfK0DyktiK"
      },
      "source": [
        "<h2>5) How you can explore and analyze continuous and discrete variables?</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uZmGa6Jk0uf"
      },
      "source": [
        "<b>Discrete variables</b> belong to a set that are distinct and separate. They are countable number of possible values, characterized by gaps or interruptions.<br><b>Continuous variables</b> belong to a set that can take any value within a specified finite or infinite interval.<br>We can analyze them with visualization graphs by looking distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4OXkNGolERT"
      },
      "source": [
        "<h2>6) What are the basic statistical terms used in machine learning?</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0jKgV0LlHxT"
      },
      "source": [
        "Correlation, covariance, mode, median, mean, standard deviation, variance, probability distributions, hypothesis testing, error functions, parameters, regression, variable, summary statistics, prediction terms are commonly used in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N6vfsYKlLHA"
      },
      "source": [
        "<h2>7) Analyse the plot given below. (What is the plot and variable type, check the distribution and make a comment about how you can preprocess it.)</h2>"
      ]
    },
    {<img src="img/hw1_graph.png"/>
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJHtk-73lTRw"
      },
      "source": [
        "Shown graph is a type of histogram. It looks like Seaborn’s Distplot. Also, a KDE plot is seen. It shows the distribution for S4 feature. S4 feature is a continuous variable. It looks like there is unbalanced deviation of the sample distribution. Some values are higher than others. Plot is divided into bins with range 0.05. In preprocessing, bucketing method can be used to minimize the effects of small observation errors (noisy data)."
      ]
    }
  ]
}
